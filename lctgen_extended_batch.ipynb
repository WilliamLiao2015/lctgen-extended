{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Impact of Structured Representations in Scenario Generation Based on Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from google.colab import userdata\n",
    "\n",
    "\n",
    "base_dir = \"/content\"\n",
    "lctgen_extended_dir = f\"{base_dir}/lctgen-extended\"\n",
    "lctgen_dir = f\"{base_dir}/lctgen\"\n",
    "\n",
    "t = userdata.get(\"GITHUB_TOKEN\")\n",
    "u = \"WilliamLiao2015\"\n",
    "r = \"lctgen-extended\"\n",
    "\n",
    "\n",
    "os.chdir(base_dir)\n",
    "!git clone https://{t}@github.com/{u}/{r}.git\n",
    "os.chdir(lctgen_extended_dir)\n",
    "os.chdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "# fetch codebase\n",
    "CODE_DIR = \"lctgen\"\n",
    "os.makedirs(f\"./{CODE_DIR}\", exist_ok=True)\n",
    "!git clone https://github.com/Ariostgx/lctgen.git $CODE_DIR\n",
    "os.chdir(lctgen_dir)\n",
    "sys.path.append(lctgen_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r requirements_colab.txt --quiet\n",
    "%pip install google-generativeai --quiet\n",
    "%pip install ipympl --quiet\n",
    "%pip install pymongo --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown https://drive.google.com/uc?id=17_TI-q4qkCOt988spWIZCqDLkZpMSptO -O data.zip\n",
    "!unzip data.zip -d {lctgen_dir}/data/demo/waymo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown https://drive.google.com/uc?id=1_s_35QO6OiHHgDxHHAa7Djadm-_I7Usr -O example.ckpt\n",
    "!mkdir {lctgen_extended_dir}/checkpoints\n",
    "!mv example.ckpt {lctgen_extended_dir}/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(lctgen_extended_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import output\n",
    "\n",
    "output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.colab import setup_colab\n",
    "\n",
    "\n",
    "setup_colab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms.openai import get_openai_llm, inference_openai_llm\n",
    "\n",
    "\n",
    "llm_name = \"gpt-3.5-turbo\" # @param [\"gpt-3.5-turbo\", \"gpt-4\"]\n",
    "llm_model = get_openai_llm(llm_name)\n",
    "\n",
    "inference_llm = lambda query: inference_openai_llm(llm_model, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Generative AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports.system import pprint\n",
    "from imports.packages import genai\n",
    "\n",
    "\n",
    "for llm_name in genai.list_models():\n",
    "    pprint(llm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms.google_generativeai import get_google_llm, inference_google_llm\n",
    "\n",
    "\n",
    "llm_name = \"gemini-1.0-pro-latest\" # @param [\"gemini-1.0-pro-latest\", \"gemini-1.5-pro-latest\"]\n",
    "llm_model = get_google_llm()\n",
    "\n",
    "inference_llm = lambda query: inference_google_llm(llm_model, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3 Together.AI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms.llama3 import inference_llama3_llm\n",
    "\n",
    "\n",
    "llm_name = \"llama3\"\n",
    "\n",
    "inference_llm = inference_llama3_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3 Local API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms.llama3_local import inference_llama3_llm\n",
    "\n",
    "\n",
    "llm_name = \"llama3 (local)\"\n",
    "\n",
    "inference_llm = inference_llama3_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from prompts.structured import scenarios, generate_prompt\n",
    "\n",
    "\n",
    "# query = 'V1 goes straight and collides with V2 while V2 turns left'  # @param {type:\"string\"}\n",
    "\n",
    "query = generate_prompt(random.choice(scenarios))\n",
    "\n",
    "print(\"Query:\")\n",
    "print(query)\n",
    "print()\n",
    "\n",
    "llm_result = inference_llm(query)\n",
    "\n",
    "print(\"LLM inference result:\")\n",
    "print(llm_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predefined LLM Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Predefined result.\"\n",
    "llm_result = \"\"\"\n",
    "Actor Vector:\n",
    "- 'V1': [-1, 0, 0, 6, 4, 3, 3, 3]\n",
    "- 'V2': [0, 0, 1, 1, 1, 1, 1, 1]\n",
    "- 'V3': [2, 0, 0, 2, 4, 4, 3, 3]\n",
    "- 'V4': [1, 0, 1, 2, 4, 4, 3, 3]\n",
    "- 'V5': [0, 1, 2, 0, 0, 0, 0, 0]\n",
    "- 'V6': [0, 1, 2, 0, 0, 0, 0, 0]\n",
    "- 'V7': [0, 1, 2, 0, 0, 0, 0, 0]\n",
    "- 'V8': [0, 1, 2, 0, 0, 0, 0, 0]\n",
    "- 'V9': [0, 1, 1, 2, 4, 4, 4, 4]\n",
    "- 'V10': [0, 1, 1, 2, 4, 4, 4, 4]\n",
    "- 'V11': [0, 1, 1, 1, 4, 4, 4, 4]\n",
    "- 'V12': [3, 1, 0, 2, 4, 4, 4, 4]\n",
    "Map Vector:\n",
    "- 'Map': [2, 2, 2, 2, 1, 2]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports.packages import np, plt, animation, HTML\n",
    "from scripts.visualize import visualize\n",
    "from utils.check_types import is_number\n",
    "\n",
    "from metrics.overlapped_area_rate import evaluate_overlapped_area_rate, visualize_overlapped_area_rate\n",
    "from metrics.road_collision_rate import evaluate_road_collision_rate, visualize_road_collision_rate\n",
    "from metrics.car_collision_rate import evaluate_car_collision_rate, visualize_car_collision_rate\n",
    "from metrics.minimum_speed_rate import evaluate_minimum_speed_rate, visualize_minimum_speed_rate\n",
    "\n",
    "\n",
    "visualizations = [\n",
    "    visualize_overlapped_area_rate,\n",
    "    visualize_road_collision_rate,\n",
    "    visualize_car_collision_rate,\n",
    "    visualize_minimum_speed_rate\n",
    "]\n",
    "evaluations = [\n",
    "    evaluate_overlapped_area_rate,\n",
    "    evaluate_road_collision_rate,\n",
    "    evaluate_car_collision_rate,\n",
    "    evaluate_minimum_speed_rate\n",
    "]\n",
    "\n",
    "\n",
    "def visualize_all(data, agents, t, visualizations=visualizations):\n",
    "    plt.gca().cla()\n",
    "    visualize(data, agents, t)\n",
    "    for i, method in enumerate(visualizations):\n",
    "        method(data, agents, t, 55 - 5 * (i + 1))\n",
    "\n",
    "def evaluate_all(data, agents, t, evaluations=evaluations):\n",
    "    results = {}\n",
    "\n",
    "    for evaluation in evaluations:\n",
    "        method_name = evaluation.__name__\n",
    "        print(f\"Evaluating method: {method_name}\")\n",
    "        results[evaluation] = []\n",
    "        for t in range(50):\n",
    "            results[evaluation].append(evaluation(data, agents, t))\n",
    "        print(evaluation.__doc__.format(result=np.mean(results[evaluation])))\n",
    "        print()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_state_str(results, query=None, llm_result=None):\n",
    "    state_str = \"\"\n",
    "    lines = json.dumps({\n",
    "        \"query\": query if query else \"\",\n",
    "        \"llm_result\": llm_result,\n",
    "        \"results\": {evaluation.__name__: values for evaluation, values in results.items()}\n",
    "    }, indent=2).splitlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        previous = is_number(lines[i - 1].strip().replace(\",\", \"\")) if i > 0 else False\n",
    "        current = is_number(line.strip().replace(\",\", \"\"))\n",
    "        next = is_number(lines[i + 1].strip().replace(\",\", \"\")) if i < len(lines) - 1 else False\n",
    "        if current: state_str += line.strip()\n",
    "        elif next: state_str += line.rstrip()\n",
    "        elif previous: state_str += line.lstrip() + \"\\n\"\n",
    "        else: state_str += line + \"\\n\"\n",
    "\n",
    "    return state_str\n",
    "\n",
    "def get_anim_html(data, agents):\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(5, 5)\n",
    "    fig.set_dpi(100)\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, lambda t: visualize_all(data, agents, t), frames=50, interval=100, repeat=False)\n",
    "    anim_html = anim.to_jshtml()\n",
    "\n",
    "    return anim_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from imports.system import os, sys, json\n",
    "from configs.paths import lctgen_dir\n",
    "sys.path.append(lctgen_dir)\n",
    "\n",
    "from imports.packages import tqdm\n",
    "from configs.demo import cfg, model, map_vecs, map_ids\n",
    "from imports.trafficgen import *\n",
    "from scripts.colab import in_colab\n",
    "from scripts.inference import inference\n",
    "from prompts.structured import scenarios, generate_prompt\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "def get_database(uri: str, database_name: str) -> MongoClient:\n",
    "    client = MongoClient(uri)\n",
    "    return client[database_name]\n",
    "\n",
    "\n",
    "if not in_colab:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "\n",
    "cfg.merge_from_list([\"DATASET.DATA_LIST.ROOT\", f\"{lctgen_dir}/data/list\"])\n",
    "cfg.merge_from_list([\"DATASET.DATA_PATH\", f\"{lctgen_dir}/data/demo/waymo\"])\n",
    "\n",
    "\n",
    "n = 1\n",
    "\n",
    "user = \"williamliao2015\"\n",
    "password = os.getenv(\"MONGODB_PASSWORD\")\n",
    "\n",
    "database = get_database(f\"mongodb+srv://{user}:{password}@cluster-1.8eayemu.mongodb.net/?retryWrites=true&w=majority&appName=Cluster-1\", \"lctgen-extended\")\n",
    "\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    clear_output()\n",
    "\n",
    "    query = generate_prompt(random.choice(scenarios))\n",
    "    llm_result = inference_llm(query).replace(\"Note: \", \"\")\n",
    "    print(\"Query:\")\n",
    "    print(query)\n",
    "    print()\n",
    "    print(\"LLM inference result:\")\n",
    "    print(llm_result)\n",
    "\n",
    "    data, agents = inference(model, cfg, map_vecs, map_ids, llm_result)\n",
    "    results = evaluate_all(data, agents, 0)\n",
    "\n",
    "    state_str = get_state_str(results, query=query, llm_result=llm_result)\n",
    "    anim_html = get_anim_html(data, agents)\n",
    "\n",
    "    database[\"batch-test\"].insert_one({\n",
    "        \"query\": query if query else \"\",\n",
    "        \"llm_name\": llm_name,\n",
    "        \"llm_result\": llm_result,\n",
    "        \"results\": {evaluation.__name__: values for evaluation, values in results.items()}\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "state_str = get_state_str(evaluate_all(data, agents, 0), query=query, llm_result=llm_result)\n",
    "\n",
    "anim_html = get_anim_html(data, agents)\n",
    "display(HTML(anim_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.paths import base_dir\n",
    "from imports.system import datetime, os\n",
    "\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "save_to = f\"{base_dir}/lctgen_records\"\n",
    "\n",
    "if not os.path.exists(save_to):\n",
    "    os.mkdir(save_to)\n",
    "\n",
    "state_dir = f\"{save_to}/{current_time}.json\"\n",
    "print(f\"Saving state to \\\"{state_dir}\\\"\")\n",
    "with open(state_dir, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(state_str)\n",
    "\n",
    "html_dir = f\"{save_to}/{current_time}.html\"\n",
    "print(f\"Saving HTML to \\\"{html_dir}\\\"\")\n",
    "with open(html_dir, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(anim_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -j {save_to}.zip {save_to}/*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
